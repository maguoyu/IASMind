import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from sklearn.neighbors import LocalOutlierFactor
from sklearn.cluster import DBSCAN
from scipy.stats import zscore, pearsonr, spearmanr
from scipy.signal import find_peaks
import pymannkendall as mk
import ruptures as rpt
from datetime import datetime
import json
import requests
from abc import ABC, abstractmethod
try:
    from src.llms.llm import get_llm_by_type
    from src.config.agents import AGENT_LLM_MAP
    HAS_LLM_CONFIG = True
except ImportError:
    HAS_LLM_CONFIG = False
    print("è­¦å‘Š: æœªæ‰¾åˆ°LLMé…ç½®ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
warnings.filterwarnings('ignore')

@dataclass
class InsightResult:
    """æ•°æ®æ´å¯Ÿç»“æœçš„æ ‡å‡†åŒ–ç»“æ„"""
    algorithm: str  # ç®—æ³•åç§°
    insight_type: str  # æ´å¯Ÿç±»å‹ï¼šanomaly, trend, correlation, etc.
    severity: str  # ä¸¥é‡ç¨‹åº¦ï¼šlow, medium, high, critical
    confidence: float  # ç½®ä¿¡åº¦ 0-1
    description: str  # æ´å¯Ÿæè¿°
    details: Dict[str, Any]  # è¯¦ç»†ç»“æœ
    affected_data: List[int] = field(default_factory=list)  # å—å½±å“çš„æ•°æ®ç´¢å¼•
    recommendations: List[str] = field(default_factory=list)  # å»ºè®®
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'algorithm': self.algorithm,
            'insight_type': self.insight_type,
            'severity': self.severity,
            'confidence': self.confidence,
            'description': self.description,
            'details': self.details,
            'affected_data': self.affected_data,
            'recommendations': self.recommendations
        }
    
    def to_friendly_description(self) -> str:
        """ç”Ÿæˆé€šä¿—æ˜“æ‡‚çš„æè¿°"""
        friendly_names = {
            'lof': 'å±€éƒ¨å¼‚å¸¸æ£€æµ‹',
            'zscore': 'æ•°å€¼å¼‚å¸¸æ£€æµ‹',
            'iqr': 'å››åˆ†ä½å¼‚å¸¸æ£€æµ‹',
            'mann_kendall': 'è¶‹åŠ¿åˆ†æ',
            'page_hinkley': 'å˜åŒ–ç‚¹æ£€æµ‹',
            'bayesian': 'è½¬æŠ˜ç‚¹åˆ†æ',
            'pearson': 'çº¿æ€§ç›¸å…³åˆ†æ',
            'spearman': 'æ’åºç›¸å…³åˆ†æ',
            'dbscan': 'èšç±»åˆ†æ',
            'cv_periodicity': 'å‘¨æœŸæ€§åˆ†æ',
            'basic_stats': 'åŸºç¡€ç»Ÿè®¡'
        }
        
        severity_icons = {
            'low': 'ğŸŸ¢',
            'medium': 'ğŸŸ¡', 
            'high': 'ğŸŸ ',
            'critical': 'ğŸ”´'
        }
        
        confidence_text = {
            (0.9, 1.0): 'éå¸¸ç¡®å®š',
            (0.7, 0.9): 'æ¯”è¾ƒç¡®å®š',
            (0.5, 0.7): 'ä¸€èˆ¬ç¡®å®š',
            (0.0, 0.5): 'ä¸å¤ªç¡®å®š'
        }
        
        # è·å–ç½®ä¿¡åº¦æ–‡æœ¬
        conf_text = 'ä¸ç¡®å®š'
        for (low, high), text in confidence_text.items():
            if low <= self.confidence < high:
                conf_text = text
                break
        
        algorithm_name = friendly_names.get(self.algorithm, self.algorithm)
        severity_icon = severity_icons.get(self.severity, 'âšª')
        
        return f"{severity_icon} [{algorithm_name}] {self.description} (ç½®ä¿¡åº¦: {conf_text})"

class LLMOptimizer(ABC):
    """å¤§æ¨¡å‹ä¼˜åŒ–å™¨åŸºç±»"""
    
    @abstractmethod
    def optimize_report(self, insights: List[InsightResult], context: str = "") -> str:
        """ä¼˜åŒ–æŠ¥å‘Šè¾“å‡º"""
        pass

class SmartLLMOptimizer(LLMOptimizer):
    """æ™ºèƒ½å¤§æ¨¡å‹ä¼˜åŒ–å™¨ - ä½¿ç”¨é¡¹ç›®é…ç½®çš„LLM"""
    
    def __init__(self):
        if HAS_LLM_CONFIG:
            try:
                self.llm = get_llm_by_type(AGENT_LLM_MAP.get("chatbot", "basic"))
                self.has_llm = True
            except Exception as e:
                print(f"LLMåˆå§‹åŒ–å¤±è´¥: {e}")
                self.has_llm = False
        else:
            self.has_llm = False
    
    def optimize_report(self, insights: List[InsightResult], context: str = "") -> str:
        """ä½¿ç”¨é…ç½®çš„LLMä¼˜åŒ–æŠ¥å‘Š"""
        if not self.has_llm:
            return self._generate_fallback_report(insights, context)
        
        try:
            # å‡†å¤‡æ´å¯Ÿæ•°æ®
            insights_text = "\n".join([
                f"- {insight.to_friendly_description()}\n  è¯¦æƒ…: {insight.details}\n  å»ºè®®: {', '.join(insight.recommendations)}"
                for insight in insights
            ])
            
            prompt = f"""
è¯·å°†ä»¥ä¸‹æ•°æ®åˆ†æç»“æœè½¬æ¢ä¸ºé€šä¿—æ˜“æ‡‚çš„ä¸šåŠ¡æŠ¥å‘Šï¼Œé¢å‘éæŠ€æœ¯èƒŒæ™¯çš„ä¸šåŠ¡äººå‘˜ï¼š

èƒŒæ™¯ä¿¡æ¯: {context}

æ•°æ®æ´å¯Ÿç»“æœ:
{insights_text}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚è¾“å‡º:
1. ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šå‘ç°çš„é—®é¢˜
2. è¯´æ˜è¿™äº›é—®é¢˜å¯¹ä¸šåŠ¡çš„æ½œåœ¨å½±å“  
3. æä¾›å…·ä½“å¯è¡Œçš„æ”¹è¿›å»ºè®®
4. é¿å…ä½¿ç”¨æŠ€æœ¯æœ¯è¯­ï¼Œå¤šç”¨æ¯”å–»å’Œå®ä¾‹
5. æŒ‰é‡è¦æ€§æ’åºå±•ç¤ºç»“æœ

è¾“å‡ºæ ¼å¼è¦æ±‚:
- ä½¿ç”¨ä¸­æ–‡
- ç»“æ„æ¸…æ™°ï¼Œæœ‰æ ‡é¢˜å’Œè¦ç‚¹
- çªå‡ºå…³é”®ä¿¡æ¯
- è¯­è¨€äº²å’Œï¼Œä¾¿äºç†è§£
"""
            response = self.llm.invoke(prompt)
            return response.content if hasattr(response, 'content') else str(response)
                
        except Exception as e:
            print(f"å¤§æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {str(e)}")
            return self._generate_fallback_report(insights, context)
    
    def _generate_fallback_report(self, insights: List[InsightResult], context: str = "") -> str:
        """ç”Ÿæˆå¤‡ç”¨æŠ¥å‘Šï¼ˆä¸ä½¿ç”¨LLMï¼‰"""
        return f"""
ğŸ“Š æ•°æ®æ´å¯Ÿåˆ†ææŠ¥å‘Š

ğŸ“‹ åˆ†æèƒŒæ™¯: {context}

ğŸ” ä¸»è¦å‘ç°:
{chr(10).join([f"â€¢ {insight.to_friendly_description()}" for insight in insights[:5]])}

ğŸ’¡ å…³é”®å»ºè®®:
{chr(10).join([f"â€¢ {rec}" for insight in insights for rec in insight.recommendations[:2]][:5])}

æ³¨: æ­¤æŠ¥å‘Šä¸ºç®€åŒ–ç‰ˆæœ¬ï¼Œå»ºè®®é…ç½®LLMè·å¾—æ›´è¯¦ç»†çš„åˆ†æã€‚
"""

class LocalLLMOptimizer(LLMOptimizer):
    """æœ¬åœ°å¤§æ¨¡å‹ä¼˜åŒ–å™¨ï¼ˆæ”¯æŒOllamaç­‰ï¼‰"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "qwen2.5"):
        self.base_url = base_url
        self.model = model
    
    def optimize_report(self, insights: List[InsightResult], context: str = "") -> str:
        """ä½¿ç”¨æœ¬åœ°å¤§æ¨¡å‹ä¼˜åŒ–æŠ¥å‘Š"""
        try:
            insights_text = "\n".join([
                f"- {insight.to_friendly_description()}\n  è¯¦æƒ…: {insight.details}\n  å»ºè®®: {', '.join(insight.recommendations)}"
                for insight in insights
            ])
            
            prompt = f"""
è¯·å°†ä»¥ä¸‹æ•°æ®åˆ†æç»“æœè½¬æ¢ä¸ºé€šä¿—æ˜“æ‡‚çš„ä¸šåŠ¡æŠ¥å‘Šï¼š

èƒŒæ™¯: {context}

å‘ç°çš„é—®é¢˜:
{insights_text}

è¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šè¿™äº›å‘ç°ï¼Œè¯´æ˜å¯¹ä¸šåŠ¡çš„å½±å“ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚é¿å…æŠ€æœ¯æœ¯è¯­ï¼Œå¤šç”¨ç”Ÿæ´»ä¸­çš„ä¾‹å­æ¥è§£é‡Šã€‚
"""
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json().get('response', 'ä¼˜åŒ–å¤±è´¥')
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            return f"æœ¬åœ°å¤§æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {str(e)}\n\nåŸå§‹æŠ¥å‘Š:\n" + "\n".join([
                insight.to_friendly_description() for insight in insights
            ])

class DataInsightFramework:
    """æ•°æ®æ´å¯Ÿæ¡†æ¶ - ç»Ÿä¸€çš„æ•°æ®åˆ†æå·¥å…·"""
    
    def __init__(self, llm_optimizer: Optional[LLMOptimizer] = None):
        self.results = []
        self.llm_optimizer = SmartLLMOptimizer()
        
        # ç®—æ³•é…ç½®
        self.config = {
            'zscore_threshold': 3.0,
            'iqr_multiplier': 1.5,
            'lof_contamination': 0.1,
            'dbscan_eps': 0.5,
            'dbscan_min_samples': 5,
            'correlation_threshold': 0.7,
            'cv_threshold': 0.3,
            'trend_alpha': 0.05
        }
    
    def clear_results(self):
        """æ¸…ç©ºä¹‹å‰çš„ç»“æœ"""
        self.results = []
    
    def _get_friendly_recommendations(self, algorithm: str, details: Dict) -> List[str]:
        """ç”Ÿæˆé€šä¿—æ˜“æ‡‚çš„å»ºè®®"""
        recommendations = []
        
        if algorithm == 'zscore':
            if details.get('outliers_count', 0) > 0:
                recommendations.extend([
                    "æ£€æŸ¥å¼‚å¸¸æ•°æ®ç‚¹æ˜¯å¦ä¸ºå½•å…¥é”™è¯¯",
                    "ç¡®è®¤æ˜¯å¦æœ‰ç‰¹æ®Šä¸šåŠ¡äº‹ä»¶å¯¼è‡´æ•°å€¼å¼‚å¸¸",
                    "è€ƒè™‘è®¾ç½®æ•°æ®éªŒè¯è§„åˆ™é¿å…æç«¯å€¼"
                ])
        
        elif algorithm == 'lof':
            if details.get('outliers_count', 0) > 0:
                recommendations.extend([
                    "å…³æ³¨å±€éƒ¨å¼‚å¸¸çš„ä¸šåŠ¡åœºæ™¯",
                    "åˆ†æå¼‚å¸¸æ•°æ®çš„å…±åŒç‰¹å¾",
                    "å»ºç«‹å¼‚å¸¸ç›‘æ§æœºåˆ¶"
                ])
        
        elif algorithm == 'mann_kendall':
            if details.get('trend') == 'increasing':
                recommendations.extend([
                    "æ•°æ®å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå¯è€ƒè™‘æ‰©å¤§æŠ•å…¥",
                    "åˆ†æä¸Šå‡åŸå› ï¼Œåˆ¶å®šå¢é•¿ç­–ç•¥",
                    "é¢„æµ‹æœªæ¥å‘å±•ï¼Œåšå¥½èµ„æºå‡†å¤‡"
                ])
            elif details.get('trend') == 'decreasing':
                recommendations.extend([
                    "æ•°æ®å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦å…³æ³¨",
                    "åˆ†æä¸‹é™åŸå› ï¼Œåˆ¶å®šæ”¹è¿›æªæ–½",
                    "åŠæ—¶è°ƒæ•´ç­–ç•¥é¿å…è¿›ä¸€æ­¥æ¶åŒ–"
                ])
        
        elif algorithm in ['pearson', 'spearman']:
            corr_value = details.get('correlation', 0)
            if abs(corr_value) > 0.7:
                if corr_value > 0:
                    recommendations.extend([
                        "ä¸¤ä¸ªæŒ‡æ ‡å¼ºæ­£ç›¸å…³ï¼Œå¯ä»¥ç›¸äº’é¢„æµ‹",
                        "ä¼˜åŒ–å…¶ä¸­ä¸€ä¸ªæŒ‡æ ‡å¯èƒ½å¸¦åŠ¨å¦ä¸€ä¸ª",
                        "å»ºç«‹è”åŠ¨ç®¡ç†æœºåˆ¶"
                    ])
                else:
                    recommendations.extend([
                        "ä¸¤ä¸ªæŒ‡æ ‡å¼ºè´Ÿç›¸å…³ï¼Œå­˜åœ¨åˆ¶çº¦å…³ç³»",
                        "éœ€è¦å¹³è¡¡ä¸¤ä¸ªæŒ‡æ ‡çš„å‘å±•",
                        "é¿å…è¿‡åº¦ä¼˜åŒ–å•ä¸€æŒ‡æ ‡"
                    ])
        
        return recommendations
    
    def _create_friendly_description(self, algorithm: str, details: Dict, severity: str) -> str:
        """ç”Ÿæˆé€šä¿—æ˜“æ‡‚çš„æè¿°"""
        
        if algorithm == 'zscore':
            outliers_count = details.get('outliers_count', 0)
            if outliers_count > 0:
                return f"å‘ç° {outliers_count} ä¸ªæ•°å€¼å¼‚å¸¸çš„æ•°æ®ç‚¹ï¼Œè¿™äº›æ•°æ®æ˜æ˜¾åç¦»æ­£å¸¸èŒƒå›´"
            else:
                return "æ•°æ®æ•°å€¼éƒ½åœ¨æ­£å¸¸èŒƒå›´å†…ï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾å¼‚å¸¸"
        
        elif algorithm == 'iqr':
            outliers_count = details.get('outliers_count', 0)
            if outliers_count > 0:
                return f"é€šè¿‡å››åˆ†ä½åˆ†æï¼Œå‘ç° {outliers_count} ä¸ªå¯èƒ½çš„å¼‚å¸¸å€¼"
            else:
                return "æ•°æ®åˆ†å¸ƒå‡åŒ€ï¼Œæ²¡æœ‰å‘ç°å¼‚å¸¸å€¼"
        
        elif algorithm == 'lof':
            outliers_count = details.get('outliers_count', 0)
            if outliers_count > 0:
                return f"å‘ç° {outliers_count} ä¸ªå±€éƒ¨å¼‚å¸¸ç‚¹ï¼Œè¿™äº›æ•°æ®åœ¨å…¶å‘¨å›´ç¯å¢ƒä¸­æ˜¾å¾—ä¸å¯»å¸¸"
            else:
                return "æ•°æ®çš„å±€éƒ¨å¯†åº¦æ­£å¸¸ï¼Œæ²¡æœ‰å‘ç°å­¤ç«‹å¼‚å¸¸"
        
        elif algorithm == 'mann_kendall':
            trend = details.get('trend', 'no trend')
            p_value = details.get('p_value', 1.0)
            if trend == 'increasing':
                return f"æ•°æ®æ•´ä½“å‘ˆç°ä¸Šå‡è¶‹åŠ¿ï¼Œç»Ÿè®¡æ˜¾è‘—æ€§å¾ˆé«˜"
            elif trend == 'decreasing':
                return f"æ•°æ®æ•´ä½“å‘ˆç°ä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦å…³æ³¨"
            else:
                return "æ•°æ®æ²¡æœ‰æ˜æ˜¾çš„ä¸Šå‡æˆ–ä¸‹é™è¶‹åŠ¿ï¼Œç›¸å¯¹ç¨³å®š"
        
        elif algorithm in ['pearson', 'spearman']:
            corr_value = details.get('correlation', 0)
            corr_type = "çº¿æ€§" if algorithm == 'pearson' else "æ’åº"
            if abs(corr_value) > 0.7:
                direction = "æ­£ç›¸å…³" if corr_value > 0 else "è´Ÿç›¸å…³"
                return f"ä¸¤ä¸ªæŒ‡æ ‡ä¹‹é—´å­˜åœ¨å¼º{direction}å…³ç³»ï¼ˆ{corr_type}ç›¸å…³ç³»æ•°: {corr_value:.3f}ï¼‰"
            elif abs(corr_value) > 0.3:
                direction = "æ­£ç›¸å…³" if corr_value > 0 else "è´Ÿç›¸å…³"
                return f"ä¸¤ä¸ªæŒ‡æ ‡ä¹‹é—´å­˜åœ¨ä¸­ç­‰ç¨‹åº¦çš„{direction}å…³ç³»"
            else:
                return "ä¸¤ä¸ªæŒ‡æ ‡ä¹‹é—´ç›¸å…³æ€§è¾ƒå¼±ï¼ŒåŸºæœ¬ç‹¬ç«‹"
        
        elif algorithm == 'dbscan':
            clusters = details.get('n_clusters', 0)
            noise_points = details.get('noise_points', 0)
            if clusters > 1:
                return f"æ•°æ®å¯ä»¥åˆ†ä¸º {clusters} ä¸ªä¸åŒçš„ç¾¤ç»„ï¼Œå¦æœ‰ {noise_points} ä¸ªå¼‚å¸¸ç‚¹"
            else:
                return f"æ•°æ®æ¯”è¾ƒåˆ†æ•£ï¼Œéš¾ä»¥å½¢æˆæ˜æ˜¾çš„ç¾¤ç»„ï¼Œæœ‰ {noise_points} ä¸ªå¼‚å¸¸ç‚¹"
        
        elif algorithm == 'cv_periodicity':
            cv = details.get('cv', 0)
            if cv > 0.5:
                return f"æ•°æ®æ³¢åŠ¨è¾ƒå¤§ï¼Œå˜å¼‚ç³»æ•°ä¸º {cv:.3f}ï¼Œå¯èƒ½å­˜åœ¨å‘¨æœŸæ€§æ¨¡å¼"
            else:
                return f"æ•°æ®ç›¸å¯¹ç¨³å®šï¼Œå˜å¼‚ç³»æ•°ä¸º {cv:.3f}"
        
        elif algorithm == 'basic_stats':
            stats = details
            return f"æ•°æ®æ¦‚å†µï¼šå¹³å‡å€¼ {stats.get('mean', 0):.2f}ï¼Œæœ€å¤§å€¼ {stats.get('max', 0):.2f}ï¼Œæœ€å°å€¼ {stats.get('min', 0):.2f}"
        
        return f"é€šè¿‡{algorithm}ç®—æ³•åˆ†æå®Œæˆ"

    def analyze_zscore(self, data: np.ndarray, threshold: float = None) -> InsightResult:
        """Z-Scoreå¼‚å¸¸æ£€æµ‹"""
        if threshold is None:
            threshold = self.config['zscore_threshold']
        
        z_scores = np.abs(zscore(data, nan_policy='omit'))
        outliers = np.where(z_scores > threshold)[0]
        
        details = {
            'threshold': threshold,
            'outliers_count': len(outliers),
            'outlier_indices': outliers.tolist(),
            'max_zscore': float(np.max(z_scores)) if len(z_scores) > 0 else 0,
            'mean_zscore': float(np.mean(z_scores)) if len(z_scores) > 0 else 0
        }
        
        severity = 'high' if len(outliers) > len(data) * 0.1 else 'medium' if len(outliers) > 0 else 'low'
        confidence = min(0.9, 0.5 + len(outliers) * 0.1)
        
        description = self._create_friendly_description('zscore', details, severity)
        recommendations = self._get_friendly_recommendations('zscore', details)
        
        return InsightResult(
            algorithm='zscore',
            insight_type='anomaly',
            severity=severity,
            confidence=confidence,
            description=description,
            details=details,
            affected_data=outliers.tolist(),
            recommendations=recommendations
        )

    def analyze_iqr(self, data: np.ndarray, multiplier: float = None) -> InsightResult:
        """IQRå¼‚å¸¸æ£€æµ‹"""
        if multiplier is None:
            multiplier = self.config['iqr_multiplier']
        
        q1, q3 = np.percentile(data, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - multiplier * iqr
        upper_bound = q3 + multiplier * iqr
        
        outliers = np.where((data < lower_bound) | (data > upper_bound))[0]
        
        details = {
            'q1': float(q1),
            'q3': float(q3),
            'iqr': float(iqr),
            'lower_bound': float(lower_bound),
            'upper_bound': float(upper_bound),
            'outliers_count': len(outliers),
            'outlier_indices': outliers.tolist()
        }
        
        severity = 'high' if len(outliers) > len(data) * 0.05 else 'medium' if len(outliers) > 0 else 'low'
        confidence = 0.8 if len(outliers) > 0 else 0.6
        
        description = self._create_friendly_description('iqr', details, severity)
        recommendations = self._get_friendly_recommendations('iqr', details)
        
        return InsightResult(
            algorithm='iqr',
            insight_type='anomaly',
            severity=severity,
            confidence=confidence,
            description=description,
            details=details,
            affected_data=outliers.tolist(),
            recommendations=recommendations
        )

    def analyze_lof(self, data: np.ndarray, contamination: float = None) -> InsightResult:
        """LOFå±€éƒ¨å¼‚å¸¸å› å­æ£€æµ‹"""
        if contamination is None:
            contamination = self.config['lof_contamination']
        
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)
        
        lof = LocalOutlierFactor(contamination=contamination, n_neighbors=min(20, len(data)-1))
        outlier_labels = lof.fit_predict(data)
        outliers = np.where(outlier_labels == -1)[0]
        
        details = {
            'contamination': contamination,
            'outliers_count': len(outliers),
            'outlier_indices': outliers.tolist(),
            'negative_outlier_factor': lof.negative_outlier_factor_.tolist()
        }
        
        severity = 'high' if len(outliers) > len(data) * 0.1 else 'medium' if len(outliers) > 0 else 'low'
        confidence = 0.85 if len(outliers) > 0 else 0.7
        
        description = self._create_friendly_description('lof', details, severity)
        recommendations = self._get_friendly_recommendations('lof', details)
        
        return InsightResult(
            algorithm='lof',
            insight_type='anomaly',
            severity=severity,
            confidence=confidence,
            description=description,
            details=details,
            affected_data=outliers.tolist(),
            recommendations=recommendations
        )

    def analyze_mann_kendall(self, data: np.ndarray, alpha: float = None) -> InsightResult:
        """Mann-Kendallè¶‹åŠ¿æ£€æµ‹"""
        if alpha is None:
            alpha = self.config['trend_alpha']
        
        result = mk.original_test(data, alpha=alpha)
        
        details = {
            'trend': result.trend,
            'h': result.h,
            'p_value': float(result.p),
            'z_score': float(result.z),
            'tau': float(result.Tau),
            'slope': float(result.slope) if hasattr(result, 'slope') else None
        }
        
        severity = 'high' if result.h and abs(result.z) > 2.5 else 'medium' if result.h else 'low'
        confidence = 1 - result.p if result.h else 0.5
        
        description = self._create_friendly_description('mann_kendall', details, severity)
        recommendations = self._get_friendly_recommendations('mann_kendall', details)
        
        return InsightResult(
            algorithm='mann_kendall',
            insight_type='trend',
            severity=severity,
            confidence=confidence,
            description=description,
            details=details,
            recommendations=recommendations
        )

    def analyze_page_hinkley(self, data: np.ndarray) -> InsightResult:
        """Page-Hinkleyå˜åŒ–ç‚¹æ£€æµ‹"""
        try:
            algo = rpt.Pelt(model="rbf").fit(data)
            change_points = algo.predict(pen=10)
            
            # ç§»é™¤æœ€åä¸€ä¸ªç‚¹ï¼ˆæ€»æ˜¯æ•°æ®é•¿åº¦ï¼‰
            if change_points and change_points[-1] == len(data):
                change_points = change_points[:-1]
            
            details = {
                'change_points': change_points,
                'change_points_count': len(change_points)
            }
            
            severity = 'high' if len(change_points) > 3 else 'medium' if len(change_points) > 0 else 'low'
            confidence = 0.8 if len(change_points) > 0 else 0.6
            
            if len(change_points) > 0:
                description = f"æ£€æµ‹åˆ° {len(change_points)} ä¸ªæ•°æ®å˜åŒ–ç‚¹ï¼Œæ•°æ®åœ¨è¿™äº›ä½ç½®å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–"
            else:
                description = "æ•°æ®ç›¸å¯¹ç¨³å®šï¼Œæ²¡æœ‰æ£€æµ‹åˆ°æ˜æ˜¾çš„å˜åŒ–ç‚¹"
            
            recommendations = []
            if len(change_points) > 0:
                recommendations.extend([
                    "åˆ†æå˜åŒ–ç‚¹å¯¹åº”çš„æ—¶é—´å’Œä¸šåŠ¡äº‹ä»¶",
                    "ç¡®è®¤å˜åŒ–æ˜¯å¦ç¬¦åˆé¢„æœŸ",
                    "å»ºç«‹å˜åŒ–ç›‘æ§æœºåˆ¶"
                ])
            
            return InsightResult(
                algorithm='page_hinkley',
                insight_type='changepoint',
                severity=severity,
                confidence=confidence,
                description=description,
                details=details,
                affected_data=change_points,
                recommendations=recommendations
            )
        except Exception as e:
            return InsightResult(
                algorithm='page_hinkley',
                insight_type='changepoint',
                severity='low',
                confidence=0.3,
                description=f"å˜åŒ–ç‚¹æ£€æµ‹å¤±è´¥: {str(e)}",
                details={'error': str(e)},
                recommendations=["æ•°æ®å¯èƒ½ä¸é€‚åˆå˜åŒ–ç‚¹æ£€æµ‹"]
            )

    def analyze_bayesian_changepoint(self, data: np.ndarray) -> InsightResult:
        """è´å¶æ–¯è½¬æŠ˜ç‚¹æ£€æµ‹"""
        try:
            algo = rpt.Dynp(model="normal", min_size=3, jump=5).fit(data)
            change_points = algo.predict(n_bkps=5)
            
            if change_points and change_points[-1] == len(data):
                change_points = change_points[:-1]
            
            details = {
                'change_points': change_points,
                'change_points_count': len(change_points),
                'method': 'bayesian_inference'
            }
            
            severity = 'high' if len(change_points) > 2 else 'medium' if len(change_points) > 0 else 'low'
            confidence = 0.85 if len(change_points) > 0 else 0.6
            
            if len(change_points) > 0:
                description = f"é€šè¿‡è´å¶æ–¯æ¨æ–­å‘ç° {len(change_points)} ä¸ªè½¬æŠ˜ç‚¹ï¼Œæ•°æ®æ¨¡å¼åœ¨è¿™äº›ä½ç½®å‘ç”Ÿæ”¹å˜"
            else:
                description = "æ•°æ®æ¨¡å¼ç›¸å¯¹ç¨³å®šï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾çš„è½¬æŠ˜ç‚¹"
            
            recommendations = []
            if len(change_points) > 0:
                recommendations.extend([
                    "é‡ç‚¹å…³æ³¨è½¬æŠ˜ç‚¹é™„è¿‘çš„ä¸šåŠ¡å˜åŒ–",
                    "åˆ†æè½¬æŠ˜åŸå› ï¼Œæ€»ç»“ç»éªŒæ•™è®­",
                    "é¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½çš„è½¬æŠ˜ç‚¹"
                ])
            
            return InsightResult(
                algorithm='bayesian',
                insight_type='changepoint',
                severity=severity,
                confidence=confidence,
                description=description,
                details=details,
                affected_data=change_points,
                recommendations=recommendations
            )
        except Exception as e:
            return InsightResult(
                algorithm='bayesian',
                insight_type='changepoint',
                severity='low',
                confidence=0.3,
                description=f"è´å¶æ–¯è½¬æŠ˜ç‚¹æ£€æµ‹å¤±è´¥: {str(e)}",
                details={'error': str(e)},
                recommendations=["æ•°æ®å¯èƒ½ä¸é€‚åˆè½¬æŠ˜ç‚¹æ£€æµ‹"]
            )

    def analyze_correlation(self, data1: np.ndarray, data2: np.ndarray, method: str = 'pearson') -> InsightResult:
        """ç›¸å…³æ€§åˆ†æ"""
        try:
            if method == 'pearson':
                corr, p_value = pearsonr(data1, data2)
            else:  # spearman
                corr, p_value = spearmanr(data1, data2)
            
            details = {
                'correlation': float(corr),
                'p_value': float(p_value),
                'method': method,
                'significant': p_value < 0.05
            }
            
            abs_corr = abs(corr)
            if abs_corr > 0.7:
                severity = 'high'
            elif abs_corr > 0.3:
                severity = 'medium'
            else:
                severity = 'low'
            
            confidence = 1 - p_value if p_value < 0.05 else 0.5
            
            description = self._create_friendly_description(method, details, severity)
            recommendations = self._get_friendly_recommendations(method, details)
            
            return InsightResult(
                algorithm=method,
                insight_type='correlation',
                severity=severity,
                confidence=confidence,
                description=description,
                details=details,
                recommendations=recommendations
            )
        except Exception as e:
            return InsightResult(
                algorithm=method,
                insight_type='correlation',
                severity='low',
                confidence=0.3,
                description=f"ç›¸å…³æ€§åˆ†æå¤±è´¥: {str(e)}",
                details={'error': str(e)},
                recommendations=["æ£€æŸ¥æ•°æ®è´¨é‡å’Œæ ¼å¼"]
            )

    def analyze_dbscan(self, data: np.ndarray, eps: float = None, min_samples: int = None) -> InsightResult:
        """DBSCANèšç±»åˆ†æ"""
        if eps is None:
            eps = self.config['dbscan_eps']
        if min_samples is None:
            min_samples = self.config['dbscan_min_samples']
        
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)
        
        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(data)
        labels = clustering.labels_
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        noise_points = list(labels).count(-1)
        
        details = {
            'n_clusters': n_clusters,
            'noise_points': noise_points,
            'eps': eps,
            'min_samples': min_samples,
            'labels': labels.tolist()
        }
        
        severity = 'high' if noise_points > len(data) * 0.1 else 'medium' if noise_points > 0 else 'low'
        confidence = 0.8 if n_clusters > 0 else 0.6
        
        description = self._create_friendly_description('dbscan', details, severity)
        recommendations = []
        if noise_points > 0:
            recommendations.extend([
                "æ£€æŸ¥å¼‚å¸¸ç‚¹æ˜¯å¦ä¸ºæ•°æ®é”™è¯¯",
                "åˆ†æå¼‚å¸¸ç‚¹çš„ä¸šåŠ¡å«ä¹‰",
                "è€ƒè™‘è°ƒæ•´èšç±»å‚æ•°"
            ])
        
        noise_indices = [i for i, label in enumerate(labels) if label == -1]
        
        return InsightResult(
            algorithm='dbscan',
            insight_type='clustering',
            severity=severity,
            confidence=confidence,
            description=description,
            details=details,
            affected_data=noise_indices,
            recommendations=recommendations
        )

    def analyze_cv_periodicity(self, data: np.ndarray) -> InsightResult:
        """å˜å¼‚ç³»æ•°å‘¨æœŸæ€§æ£€æµ‹"""
        cv = np.std(data) / np.mean(data) if np.mean(data) != 0 else float('inf')
        
        details = {
            'cv': float(cv),
            'mean': float(np.mean(data)),
            'std': float(np.std(data))
        }
        
        if cv > 0.5:
            severity = 'high'
        elif cv > 0.3:
            severity = 'medium'
        else:
            severity = 'low'
        
        confidence = 0.7
        
        description = self._create_friendly_description('cv_periodicity', details, severity)
        
        recommendations = []
        if cv > 0.3:
            recommendations.extend([
                "æ•°æ®æ³¢åŠ¨è¾ƒå¤§ï¼Œåˆ†ææ³¢åŠ¨åŸå› ",
                "è€ƒè™‘æ˜¯å¦å­˜åœ¨å‘¨æœŸæ€§æ¨¡å¼",
                "å»ºç«‹æ³¢åŠ¨ç›‘æ§æœºåˆ¶"
            ])
        
        return InsightResult(
            algorithm='cv_periodicity',
            insight_type='periodicity',
            severity=severity,
            confidence=confidence,
            description=description,
            details=details,
            recommendations=recommendations
        )

    def analyze_basic_stats(self, data: np.ndarray) -> InsightResult:
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        stats = {
            'count': len(data),
            'mean': float(np.mean(data)),
            'median': float(np.median(data)),
            'std': float(np.std(data)),
            'min': float(np.min(data)),
            'max': float(np.max(data)),
            'range': float(np.max(data) - np.min(data)),
            'q1': float(np.percentile(data, 25)),
            'q3': float(np.percentile(data, 75))
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸çš„ç»Ÿè®¡ç‰¹å¾
        cv = stats['std'] / stats['mean'] if stats['mean'] != 0 else float('inf')
        range_ratio = stats['range'] / stats['mean'] if stats['mean'] != 0 else float('inf')
        
        severity = 'medium' if cv > 1 or range_ratio > 5 else 'low'
        confidence = 0.9
        
        description = self._create_friendly_description('basic_stats', stats, severity)
        
        recommendations = []
        if cv > 1:
            recommendations.append("æ•°æ®å˜å¼‚è¾ƒå¤§ï¼Œéœ€è¦æ·±å…¥åˆ†æåŸå› ")
        if range_ratio > 5:
            recommendations.append("æ•°æ®èŒƒå›´å¾ˆå¤§ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æç«¯å€¼")
        
        return InsightResult(
            algorithm='basic_stats',
            insight_type='descriptive',
            severity=severity,
            confidence=confidence,
            description=description,
            details=stats,
            recommendations=recommendations
        )

    def analyze(self, data: Union[pd.DataFrame, pd.Series, np.ndarray], 
                column: str = None, 
                algorithms: List[str] = None,
                **kwargs) -> List[InsightResult]:
        """ç»Ÿä¸€åˆ†ææ¥å£"""
        self.clear_results()
        
        # æ•°æ®é¢„å¤„ç†
        if isinstance(data, pd.DataFrame):
            if column is None:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) == 0:
                    raise ValueError("DataFrameä¸­æ²¡æœ‰æ•°å€¼åˆ—")
                column = numeric_cols[0]
            analysis_data = data[column].dropna().values
        elif isinstance(data, pd.Series):
            analysis_data = data.dropna().values
        else:
            analysis_data = np.array(data)
            analysis_data = analysis_data[~np.isnan(analysis_data)]
        
        if len(analysis_data) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¿›è¡Œåˆ†æ")
        
        # é»˜è®¤ç®—æ³•åˆ—è¡¨
        if algorithms is None:
            algorithms = ['basic_stats', 'zscore', 'iqr', 'mann_kendall']
            if len(analysis_data) > 10:
                algorithms.extend(['lof', 'cv_periodicity'])
            if len(analysis_data) > 20:
                algorithms.extend(['page_hinkley', 'bayesian', 'dbscan'])
        
        # è¿è¡Œåˆ†æç®—æ³•
        for algorithm in algorithms:
            try:
                if algorithm == 'zscore':
                    result = self.analyze_zscore(analysis_data, **kwargs)
                elif algorithm == 'iqr':
                    result = self.analyze_iqr(analysis_data, **kwargs)
                elif algorithm == 'lof':
                    result = self.analyze_lof(analysis_data, **kwargs)
                elif algorithm == 'mann_kendall':
                    result = self.analyze_mann_kendall(analysis_data, **kwargs)
                elif algorithm == 'page_hinkley':
                    result = self.analyze_page_hinkley(analysis_data)
                elif algorithm == 'bayesian':
                    result = self.analyze_bayesian_changepoint(analysis_data)
                elif algorithm == 'dbscan':
                    result = self.analyze_dbscan(analysis_data, **kwargs)
                elif algorithm == 'cv_periodicity':
                    result = self.analyze_cv_periodicity(analysis_data)
                elif algorithm == 'basic_stats':
                    result = self.analyze_basic_stats(analysis_data)
                else:
                    continue
                
                self.results.append(result)
            except Exception as e:
                print(f"ç®—æ³• {algorithm} æ‰§è¡Œå¤±è´¥: {str(e)}")
        
        return self.results

    def analyze_correlation_pair(self, data: pd.DataFrame, col1: str, col2: str, 
                                methods: List[str] = None) -> List[InsightResult]:
        """åˆ†æä¸¤ä¸ªå˜é‡çš„ç›¸å…³æ€§"""
        if methods is None:
            methods = ['pearson', 'spearman']
        
        data1 = data[col1].dropna().values
        data2 = data[col2].dropna().values
        
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        min_len = min(len(data1), len(data2))
        data1 = data1[:min_len]
        data2 = data2[:min_len]
        
        results = []
        for method in methods:
            result = self.analyze_correlation(data1, data2, method)
            results.append(result)
            self.results.append(result)
        
        return results

    def get_summary_report(self, context: str = "") -> str:
        """ç”Ÿæˆé€šä¿—æ˜“æ‡‚çš„æ‘˜è¦æŠ¥å‘Š"""
        if not self.results:
            return "æš‚æ— åˆ†æç»“æœ"
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        severity_order = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}
        sorted_results = sorted(self.results, 
                              key=lambda x: (severity_order.get(x.severity, 0), x.confidence), 
                              reverse=True)
        
        # å¦‚æœé…ç½®äº†å¤§æ¨¡å‹ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨å®ƒæ¥ä¼˜åŒ–æŠ¥å‘Š
        if self.llm_optimizer:
            try:
                return self.llm_optimizer.optimize_report(sorted_results, context)
            except Exception as e:
                print(f"å¤§æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
                # ç»§ç»­ä½¿ç”¨é»˜è®¤æŠ¥å‘Š
        
        # é»˜è®¤æŠ¥å‘Šæ ¼å¼
        report = ["ğŸ“Š æ•°æ®æ´å¯ŸæŠ¥å‘Š", "=" * 50, ""]
        
        if context:
            report.extend([f"ğŸ“‹ åˆ†æèƒŒæ™¯: {context}", ""])
        
        # é‡è¦å‘ç°
        high_severity = [r for r in sorted_results if r.severity in ['critical', 'high']]
        if high_severity:
            report.extend(["ğŸš¨ é‡è¦å‘ç°:", ""])
            for i, result in enumerate(high_severity, 1):
                report.append(f"{i}. {result.to_friendly_description()}")
                if result.recommendations:
                    report.append(f"   ğŸ’¡ å»ºè®®: {'; '.join(result.recommendations[:2])}")
                report.append("")
        
        # ä¸€èˆ¬å‘ç°
        medium_severity = [r for r in sorted_results if r.severity == 'medium']
        if medium_severity:
            report.extend(["ğŸ“ˆ ä¸€èˆ¬å‘ç°:", ""])
            for i, result in enumerate(medium_severity, 1):
                report.append(f"{i}. {result.to_friendly_description()}")
                report.append("")
        
        # åŸºç¡€ä¿¡æ¯
        low_severity = [r for r in sorted_results if r.severity == 'low']
        if low_severity:
            report.extend(["ğŸ“‹ åŸºç¡€ä¿¡æ¯:", ""])
            for i, result in enumerate(low_severity, 1):
                report.append(f"{i}. {result.to_friendly_description()}")
                report.append("")
        
        # æ€»ç»“å»ºè®®
        all_recommendations = []
        for result in high_severity + medium_severity:
            all_recommendations.extend(result.recommendations)
        
        if all_recommendations:
            unique_recommendations = list(dict.fromkeys(all_recommendations))[:5]  # å»é‡å¹¶é™åˆ¶æ•°é‡
            report.extend(["ğŸ’¡ æ€»ä½“å»ºè®®:", ""])
            for i, rec in enumerate(unique_recommendations, 1):
                report.append(f"{i}. {rec}")
            report.append("")
        
        report.extend(["", f"ğŸ“… åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"])
        
        return "\n".join(report)

    def export_results(self, format: str = 'json') -> Union[str, Dict]:
        """å¯¼å‡ºç»“æœ"""
        if format == 'json':
            return json.dumps([result.to_dict() for result in self.results], 
                            indent=2, ensure_ascii=False, default=str)
        elif format == 'dict':
            return [result.to_dict() for result in self.results]
        else:
            raise ValueError("æ”¯æŒçš„æ ¼å¼: 'json', 'dict'")

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', periods=100, freq='D')
    sales = 1000 + np.linspace(0, 200, 100) + np.random.normal(0, 30, 100)
    sales[20] = 2000  # å¼‚å¸¸é«˜å€¼
    sales[50] = 200   # å¼‚å¸¸ä½å€¼
    
    data = pd.DataFrame({
        'date': dates,
        'sales': sales,
        'advertising': sales * 0.1 + np.random.normal(0, 10, 100)
    })
    
    # æµ‹è¯•åŸºç¡€æ¡†æ¶
    print("=== æµ‹è¯•åŸºç¡€æ•°æ®æ´å¯Ÿæ¡†æ¶ ===")
    framework = DataInsightFramework()
    insights = framework.analyze(data, column='sales')
    
    print(framework.get_summary_report("ç”µå•†å¹³å°æ—¥é”€å”®é¢åˆ†æ"))
    
    # æµ‹è¯•ç›¸å…³æ€§åˆ†æ
    print("\n=== æµ‹è¯•ç›¸å…³æ€§åˆ†æ ===")
    corr_insights = framework.analyze_correlation_pair(data, 'sales', 'advertising')
    
    for insight in corr_insights:
        print(insight.to_friendly_description()) 